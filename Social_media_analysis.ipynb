{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "i have this aim from this analysis. first of all\n",
        "What type of posts are engaging? - Engagement Analysis\n",
        "What are the major topics for these posts? - Topic Analysis\n",
        "Which topics are the most and least engaging?\n",
        "\n",
        "to this i scrap different sources for burgerme for example the facebook and also the google forum.\n",
        "\n",
        "*   Create an engagement scores by taking weighted average of likes and comments for the posts\n",
        "*   Identify the topics for top and bottom quartile"
      ],
      "metadata": {
        "id": "1I-hMidat-qz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I80KF-Idt7Pz"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "import pandas as pd\n",
        "Nf_1 = pd.read_csv(\"/content/facebook_Kiel.csv\")\n",
        "Nf_1.columns\n",
        "Nf_1.drop(['link', 'share', 'time'], axis=1, inplace= True )\n",
        "\n",
        "Nf_2 = pd.read_csv(\"/content/google (1).csv\")\n",
        "Nf_2.drop(['X8zlde', 'JKPXdf', 'JKPXdf href', 'KgoSse', 'KgoSse href', 'YLpWtd', 'e7TeLd href', 'rbm24b' ], axis=1, inplace= True )\n",
        "\n",
        "#  attach two table together and fill the null in the comments column.\n",
        "data=pd.concat([Nf_1, Nf_2], axis=0)\n",
        "data['comments 1'].fillna('ist', inplace=True)\n",
        "data['comments 2'].fillna('ist', inplace=True)\n",
        "data['post description 1'].fillna('ist', inplace=True)\n",
        "data['post description 2'].fillna('ist', inplace=True)\n",
        "data['post description 3'].fillna('ist', inplace=True)\n",
        "data['post description 4'].fillna('ist', inplace=True)\n",
        "data['Vz1Vkc'].fillna('ist', inplace=True)\n",
        "data['QRCoQb'].fillna('0', inplace=True)\n",
        "data['like'].fillna('0', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we need english text for topic modeling,\n",
        "i creat english comment columns:"
      ],
      "metadata": {
        "id": "gQiF2avXue__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "data['comments 1_en'] = data['comments 1'].apply(lambda x: translator.translate(x, src='de', dest='en').text if x is not None else \"\")\n",
        "data['comments 2_en'] = data['comments 2'].apply(lambda x: translator.translate(x, src='de', dest='en').text if x is not None else \"\")\n",
        "data['post description 1_en'] = data['post description 1'].apply(lambda x: translator.translate(x, src='de', dest='en').text if x is not None else \"\")\n",
        "data['post description 2_en'] = data['post description 2'].apply(lambda x: translator.translate(x, src='de', dest='en').text if x is not None else \"\")\n",
        "data['post description 3_en'] = data['post description 3'].apply(lambda x: translator.translate(x, src='de', dest='en').text if x is not None else \"\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "u4hgJ4rnuqTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doing preprocessing NLP methods:**\n",
        "\n",
        "\n",
        "*   **Tokenization**\n",
        "*   **Removing Stopwords and Special Characters (Using re)**\n",
        "*   **Lemmatization**\n"
      ],
      "metadata": {
        "id": "wu3Wer0ZuyTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For first analysis we use the german pre-trained model."
      ],
      "metadata": {
        "id": "ZgVQRjovu1up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "8OzNARxXu1Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "import spacy\n",
        "# load a pretrained german model for our anlaysis\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "# use our model for column comment 1\n",
        "data['comments 1'] = data['comments 1'].apply(lambda x: nlp(x))\n",
        "data['comments 2'] = data['comments 2'].apply(lambda x: nlp(x))\n",
        "data['Vz1Vkc']= data['Vz1Vkc'].apply(lambda x: nlp(x))\n",
        "\n",
        "#remove stopwords from comments\n",
        "import nltk as n\n",
        "from nltk.corpus import stopwords  # Import stopwords\n",
        "n.download('stopwords')\n",
        "\n",
        "# Modified to handle spaCy Doc objects\n",
        "data['comments 1'] = data['comments 1'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('german'))]))\n",
        "data['comments 2'] = data['comments 2'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('german'))]))\n",
        "data['Vz1Vkc'] = data['Vz1Vkc'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('german'))]))\n",
        "\n",
        "#remove sth like ! from comments\n",
        "import re\n",
        "data['comments 1'] = data['comments 1'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "data['comments 2'] = data['comments 2'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "data['Vz1Vkc'] = data['Vz1Vkc'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# Nf_1['comments 1'] now contains strings,\n",
        "# we need to convert them back to spaCy Doc objects\n",
        "data['comments 1'] = data['comments 1'].apply(lambda text: nlp(text))\n",
        "data['comments 2'] = data['comments 2'].apply(lambda text: nlp(text))\n",
        "data['Vz1Vkc'] = data['Vz1Vkc'].apply(lambda text: nlp(text))\n",
        "# Now we can perform lemmatization\n",
        "data['comments 1'] = data['comments 1'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "data['comments 2'] = data['comments 2'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "data['Vz1Vkc'] = data['Vz1Vkc'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "\n"
      ],
      "metadata": {
        "id": "tlbZQN0kuxzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Analysis*\n",
        "## Basic Sentiment\n",
        "you can use sentiment analysis to gauge the general positivity or negativity of each comment and combine that with engagement metrics like the number of likes to create a scoring system."
      ],
      "metadata": {
        "id": "N2dPj8qKvasP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "# define a function for engagment comments text\n",
        "def sentimentfunction(text):\n",
        "    blob = TextBlob(text)\n",
        "    #return 0, 1, -1 score\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "data['sentiment_score'] = data['comments 1'].apply(sentimentfunction)\n",
        "data['sentiment_score 2'] = data['comments 2'].apply(sentimentfunction)\n",
        "data['sentiment_score 3'] = data['Vz1Vkc'].apply(sentimentfunction)\n",
        "data[ 'avarage_sentiment_score'] = data[['sentiment_score', 'sentiment_score 2', 'sentiment_score 3']].mean(axis=1)\n",
        "data['like'] = pd.to_numeric(data['like'], errors='coerce')\n",
        "data['like'] = data['like'].fillna(0)\n",
        "data['sentiment_score'] = data['avarage_sentiment_score'].fillna(0)\n",
        "data['engagment score']= data['like']*data['avarage_sentiment_score']"
      ],
      "metadata": {
        "id": "CqnXOvYPvaXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_score = data[data['avarage_sentiment_score'] != 0]\n",
        "sentiment_score[\"avarage_sentiment_score\"]"
      ],
      "metadata": {
        "id": "0LZ5BTgJviGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result will be sth like\n",
        "\n",
        "\n",
        "```\n",
        "avarage_sentiment_score\n",
        "3\t0.266667\n",
        "35\t0.111111\n",
        "0\t0.177778\n",
        "1\t0.048810\n",
        "2\t-0.083983\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Py4kds32vkrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engagment_score = data[data['engagment score'] != 0]\n",
        "engagment_score[\"engagment score\"]\n"
      ],
      "metadata": {
        "id": "V9urAaXcvuRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result will be sth like\n",
        "\n",
        "\n",
        "```\n",
        "engagment score\n",
        "3\t0.830000\n",
        "7 0.112000\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "and by this we find Tone of voice is positive or negative.\n"
      ],
      "metadata": {
        "id": "dyOEHBjOwBEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Analysis"
      ],
      "metadata": {
        "id": "jIeZeP31wcFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "GxiXCo6CwPV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eleminate the stopwords # i use only post description 1 and 2 for topic clustring since i relize that 3 is meangless\n",
        "# use our model for column comment 1\n",
        "nlpen=spacy.load(\"en_core_web_md\")\n",
        "data['comments 1_en'] = data['comments 1_en'].apply(lambda x: nlpen(x))\n",
        "data['comments 2_en'] = data['comments 2_en'].apply(lambda x: nlpen(x))\n",
        "data['post description 1_en'] = data['post description 1_en'].apply(lambda x: nlpen(x))\n",
        "data['post description 2_en'] = data['post description 2_en'].apply(lambda x: nlpen(x))\n",
        "from nltk.corpus import stopwords  # Import stopwords\n",
        "n.download('stopwords')\n",
        "\n",
        "# Modified to handle spaCy Doc objects\n",
        "data['comments 1_en'] = data['comments 1_en'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('english'))]))\n",
        "data['comments 2_en'] = data['comments 2_en'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('english'))]))\n",
        "data['post description 1_en'] = data['post description 1_en'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('english'))]))\n",
        "data['post description 2_en'] = data['post description 2_en'].apply(lambda comment: ' '.join([token.text for token in comment if token.text.lower() not in set(stopwords.words('english'))]))\n",
        "\n",
        "#remove sth like ! from comments\n",
        "import re\n",
        "data['comments 1_en'] = data['comments 1_en'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "data['comments 2_en'] = data['comments 2_en'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "data['post description 1_en'] = data['post description 1_en'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "data['post description 2_en'] = data['post description 2_en'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# lemmit.\n",
        "data['comments 1_en'] = data['comments 1_en'].apply(lambda text: nlp(text))\n",
        "data['comments 2_en'] = data['comments 2_en'].apply(lambda text: nlp(text))\n",
        "data['post description 1_en'] = data['post description 1_en'].apply(lambda text: nlp(text))\n",
        "data['post description 2_en'] = data['post description 2_en'].apply(lambda text: nlp(text))\n",
        "# Now we can perform lemmatization\n",
        "data['comments 1_en'] = data['comments 1_en'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "data['comments 2_en'] = data['comments 2_en'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "data['post description 1_en'] = data['post description 1_en'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "data['post description 2_en'] = data['post description 2_en'].apply(lambda doc: ' '.join([token.lemma_ for token in doc]))\n",
        "\n",
        "\n",
        "\n",
        "#retokenize it again to use embedding\n",
        "data['comments 1_en'] = data['comments 1_en'].apply(lambda x: nlpen(x))\n",
        "data['comments 2_en'] = data['comments 2_en'].apply(lambda x: nlpen(x))\n",
        "data['post description 1_en'] = data['comments 2_en'].apply(lambda x: nlpen(x))\n",
        "data['post description 2_en'] = data['comments 2_en'].apply(lambda x: nlpen(x))\n"
      ],
      "metadata": {
        "id": "wow5SSiZwj_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use **k-mean** method for topic modeling"
      ],
      "metadata": {
        "id": "gnkw2ASzw0UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "nlpen=spacy.load(\"en_core_web_md\")\n",
        "#combine the text of comments\n",
        "data['commenpost_combined'] = data['comments 1_en'].apply(lambda doc: doc.text) + ' ' + data['comments 2_en'].apply(lambda doc: doc.text)+ ' ' + data['post description 1_en'].apply(lambda doc: doc.text) + ' ' + data['post description 2_en'].apply(lambda doc: doc.text)\n",
        "# do embedding\n",
        "data['embeding_comment']= data['commenpost_combined'].apply(lambda x:nlpen(x).vector)\n",
        "embedding_matrix=data['embeding_comment'].to_list()\n"
      ],
      "metadata": {
        "id": "0clkh4Z0wuq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a range of number of clustering group a range between 1 to 10\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "# Initialize inertias as an empty list to store inertia values\n",
        "inertias = []\n",
        "K_v= range(1, 10)\n",
        "for k in K_v:\n",
        "  kmeans = KMeans(n_clusters=k, random_state=0).fit(embedding_matrix)\n",
        "  inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(K_v, inertias, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal K')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Ql_SxmLxCJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By our **Elbow diagram** i realize that from 3 or 4 cluster group we have no especial inertia decreasing and means that i choose the number of clustering as 4."
      ],
      "metadata": {
        "id": "-hEVgBEnxIU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#doing final k-means analysis\n",
        "numcluster = 4\n",
        "kmeans = KMeans(n_clusters=numcluster, random_state=0).fit(embedding_matrix)\n",
        "data['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "1wVeL7htxPEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensionality for visualization\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=data['cluster'], cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.title(\"Post Topics Clustering\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3qSlV--1xS8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['cluster'].value_counts()"
      ],
      "metadata": {
        "id": "cHtt1s7Cxd9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the result will be sth like this\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "cluster\n",
        "0\tn_1\n",
        "1\tn_2\n",
        "2\tn_3\n",
        "3\tn_4\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XU5haYYGxic8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#form green here i mean purple group because i have correct a mistake and the color changed. but i have not change the code name.\n",
        "green_label =0\n",
        "green_post = data[data['cluster']== green_label]"
      ],
      "metadata": {
        "id": "Cq1o30Kjx2CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "1ptGPgW6yATk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verctorization"
      ],
      "metadata": {
        "id": "wQ2o6VVxyGuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer= CountVectorizer(stop_words = 'english', max_features=5000)\n",
        "#Document-Term Matrix (DTM)\n",
        "X= vectorizer.fit_transform(green_post['commenpost_combined'])"
      ],
      "metadata": {
        "id": "k088HEGEyHSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic modeling\n",
        "by different topic number"
      ],
      "metadata": {
        "id": "hkP_w3wqyKFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = 3\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "IUsBVY9ByNOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_name, no_top_words):\n",
        "    topics = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        topic_keywords = [feature_name[i] for i in topic.argsort()[:-no_top_words - 1:-1]] # Fixed indentation, and change feature_names to feature_name\n",
        "        print(\" \".join(topic_keywords))\n",
        "        topics[\"Topic %d:\" % (topic_idx)] = \" \".join(topic_keywords)\n",
        "    return topics"
      ],
      "metadata": {
        "id": "_5awroM5yQhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recive the names from vectorize\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topics = display_topics(lda, feature_names, 10)"
      ],
      "metadata": {
        "id": "m8uPU3Q_yTWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the result will be sth like this\n",
        "\n",
        "\n",
        "```\n",
        "Topic 0:\n",
        "unfortunately time come silke yes storeen self shame larissa hope\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DlSAAranyXzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = 5\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
        "lda.fit(X)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topics = display_topics(lda, feature_names, 10)\n",
        "\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
        "lda.fit(X)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topics = display_topics(lda, feature_names, 10)\n",
        "\n",
        "top_comments = green_post.nlargest(10, 'engagment score')\n",
        "print(\"Top 10 Comments by Engagement in purple Cluster:\")\n",
        "print(top_comments[['commenpost_combined', 'engagment score']])\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "fig, axes = plt.subplots(1, n_topics, figsize=(15, 5))\n",
        "for i, topic in enumerate(topics.keys()):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(topics[topic]))\n",
        "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
        "    axes[i].axis(\"off\")\n",
        "    axes[i].set_title(f\"{topic}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HTptdo_byixi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Topic Analysis and Interpretation**\n",
        "by the relation of topic cluster kezwords and engagment score we can intrpret the social media.\n",
        "\n",
        "\n",
        "\n",
        "*  **High Engagement Areas**\n",
        "*  **Customer Sentiment**\n",
        "\n"
      ],
      "metadata": {
        "id": "NyuTEKcizBgC"
      }
    }
  ]
}